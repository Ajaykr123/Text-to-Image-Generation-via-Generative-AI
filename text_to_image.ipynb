{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT_3DdO9k2b4",
        "outputId": "6e514ed0-9c80-4ff1-c35a-7f667846e2fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive;\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ-sP4HilC3J",
        "outputId": "24928105-9e6c-4a6b-da1b-7f6b76ada851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train2017.zip       100%[===================>]  18.01G  17.0MB/s    in 11m 11s \n",
            "annotations_trainva 100%[===================>] 241.19M  55.5MB/s    in 4.8s    \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p coco_dataset/train2017 coco_dataset/annotations\n",
        "\n",
        "# Download train2017 images (~18GB, but we'll fetch only a subset)\n",
        "!wget -q --show-progress http://images.cocodataset.org/zips/train2017.zip\n",
        "!unzip -q train2017.zip -d coco_dataset/\n",
        "\n",
        "# Download annotations (~250MB)\n",
        "!wget -q --show-progress http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip -q annotations_trainval2017.zip -d coco_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0jy09IToViF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "\n",
        "class COCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, ann_file, transform=None, max_images=19700):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = COCO(ann_file)\n",
        "        self.ids = list(self.coco.anns.keys())[:max_images]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann_id = self.ids[idx]\n",
        "        caption = self.coco.anns[ann_id]['caption']\n",
        "        img_id = self.coco.anns[ann_id]['image_id']\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        path = img_info['file_name']\n",
        "        img_path = os.path.join(self.img_dir, path)\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except (OSError, UnidentifiedImageError):\n",
        "            return None, None\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, caption\n",
        "\n",
        "\n",
        "# Define TextEncoder\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self.bert(text)['last_hidden_state']\n",
        "        features = self.linear(encoded_layers[:, 0, :])\n",
        "        return features\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, text_dim, noise_dim, img_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(text_dim + noise_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, img_size * img_size * 3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, text, noise):\n",
        "        x = torch.cat([text, noise], 1)\n",
        "        img = self.fc(x)\n",
        "        img = img.view(img.size(0), 3, img_size, img_size)\n",
        "        return img\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size, text_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.fc = nn.Linear(256 * 8 * 8 + text_dim, 1)\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        x = self.conv(img)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.cat([x, text], 1)\n",
        "        validity = self.fc(x)\n",
        "        return validity\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "# Hyperparameters\n",
        "text_dim = 768\n",
        "noise_dim = 100\n",
        "img_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "epochs = 5\n",
        "\n",
        "#save model here\n",
        "save_dir = \"/content/gdrive/MyDrive/dfgan\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_model_path = os.path.join(save_dir, \"dfgan_model.pth\")\n",
        "\n",
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "\n",
        "dataset = COCODataset(\n",
        "    img_dir='coco_dataset/train2017',\n",
        "    ann_file='coco_dataset/annotations/captions_train2017.json',\n",
        "    transform=transform,\n",
        "    max_images=19700\n",
        ")\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize tokenizer and text encoder\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text_encoder = TextEncoder(text_dim)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(text_dim, noise_dim, img_size)\n",
        "discriminator = Discriminator(img_size, text_dim)\n",
        "\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "# Initialize lists to track losses\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs, captions) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)):\n",
        "        if imgs is None or captions is None:\n",
        "            continue\n",
        "\n",
        "        batch_size = imgs.size(0)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = torch.ones(batch_size, 1, requires_grad=False)\n",
        "        fake = torch.zeros(batch_size, 1, requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = imgs.type(torch.FloatTensor)\n",
        "        encoded_text = tokenizer(captions, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size, noise_dim)\n",
        "        gen_text = text_encoder(encoded_text)\n",
        "        gen_imgs = generator(gen_text, z)\n",
        "\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs, gen_text), valid)\n",
        "\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs, gen_text), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_text), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Save losses for plotting later\n",
        "        G_losses.append(g_loss.item())\n",
        "        D_losses.append(d_loss.item())\n",
        "\n",
        "        tqdm.write(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save({\n",
        "    'generator_state_dict': generator.state_dict(),\n",
        "    'discriminator_state_dict': discriminator.state_dict(),\n",
        "    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "}, save_model_path)\n",
        "\n",
        "print(\"Training finished and model saved.\")\n",
        "\n",
        "# Plot the training losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses, label=\"G\")\n",
        "plt.plot(D_losses, label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1jIXGsE4qAsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud4ff-rBLxK2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.utils import make_grid\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define TextEncoder, Generator, and other required classes (as in train_dfgan.py)\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        with torch.no_grad():\n",
        "            encoded_layers = self.bert(text)['last_hidden_state']\n",
        "        features = self.linear(encoded_layers[:, 0, :])\n",
        "        return features\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, text_dim, noise_dim, img_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(text_dim + noise_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, img_size * img_size * 3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, text, noise):\n",
        "        x = torch.cat([text, noise], 1)\n",
        "        img = self.fc(x)\n",
        "        img = img.view(img.size(0), 3, img_size, img_size)\n",
        "        return img\n",
        "\n",
        "# Hyperparameters\n",
        "text_dim = 768\n",
        "noise_dim = 100\n",
        "img_size = 64\n",
        "\n",
        "save_dir = \"/content/gdrive/MyDrive/dfgan\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_model_path = os.path.join(save_dir, \"dfgan_model.pth\")\n",
        "\n",
        "# Load the model\n",
        "checkpoint = torch.load(save_model_path, map_location=device)\n",
        "\n",
        "# Initialize tokenizer and text encoder\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text_encoder = TextEncoder(text_dim).to(device)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(text_dim, noise_dim, img_size).to(device)\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "generator.eval()\n",
        "\n",
        "def generate_image_from_text(text):\n",
        "    encoded_text = tokenizer([text], return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
        "    gen_text = text_encoder(encoded_text)\n",
        "    z = torch.randn(1, noise_dim).to(device)\n",
        "    gen_img = generator(gen_text, z)\n",
        "    return gen_img\n",
        "\n",
        "# Example usage\n",
        "text_description = \"text written\"\n",
        "generated_image = generate_image_from_text(text_description)\n",
        "\n",
        "# Plot the generated image\n",
        "plt.imshow(make_grid(generated_image, normalize=True).permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gkde8R7JkgO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}